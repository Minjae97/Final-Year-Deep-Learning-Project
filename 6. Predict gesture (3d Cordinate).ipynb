{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "happy-anniversary",
   "metadata": {},
   "source": [
    "# motion_repr_learning/ae/utils/flags.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "supported-consensus",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<absl.flags._flagvalues.FlagHolder at 0x2204f881108>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "This module contrains all the flags for the motion representation learning repository\n",
    "\"\"\"\n",
    "from __future__ import division\n",
    "import os\n",
    "from os.path import join as pjoin\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "os.environ['HOME'] = 'H:\\\\minjae\\\\Trying Obama Dataset'\n",
    "\n",
    "# Modify this function to set your home directory for this repo\n",
    "def home_out(path):\n",
    "    return pjoin(os.environ['HOME'], 'tmp', 'MoCap', path)\n",
    "\n",
    "flags = tf.app.flags\n",
    "FLAGS = flags.FLAGS\n",
    "tf.app.flags.DEFINE_string('f', '', 'kernel')# added, no error\n",
    "\n",
    "\n",
    "\"\"\"  \t\t\t\t\t\t\tFine-tuning Parameters \t\t\t\t\"\"\"\n",
    "\n",
    "#                       Flags about the sequence processing\n",
    "\n",
    "flags.DEFINE_integer('chunk_length', 1, 'Length of the chunks, for the data processing.')\n",
    "\n",
    "#                               Flags about training\n",
    "flags.DEFINE_float('learning_rate', 0.0001,\n",
    "                   'learning rate for training .')\n",
    "flags.DEFINE_float('pretraining_learning_rate', 0.001 ,\n",
    "                   'learning rate for training .')\n",
    "\n",
    "flags.DEFINE_float('variance_of_noise', 0.05, 'Coefficient for the gaussian noise '\n",
    "                                              'added to every point in input during the training')\n",
    "\n",
    "flags.DEFINE_boolean('pretrain', False,' Whether we pretrain the model in a layerwise way')\n",
    "flags.DEFINE_boolean('restore', False,' Whether we restore the model from the checkpoint')\n",
    "\n",
    "flags.DEFINE_boolean('evaluate', False, ' Whether we are evaluating the system')\n",
    "\n",
    "flags.DEFINE_float('dropout', 0.9, 'Probability to keep the neuron on')\n",
    "\n",
    "flags.DEFINE_integer('batch_size', 128,\n",
    "                     'Size of the mini batch')\n",
    "\n",
    "flags.DEFINE_integer('training_epochs', 20,\n",
    "                     \"Number of training epochs for pretraining layers\")\n",
    "flags.DEFINE_integer('pretraining_epochs', 5,\n",
    "                     \"Number of training epochs for pretraining layers\")\n",
    "\n",
    "flags.DEFINE_float('weight_decay', 0.5, ' Whether we apply weight decay')\n",
    "\n",
    "flags.DEFINE_boolean('early_stopping', True, ' Whether we do early stopping')\n",
    "flags.DEFINE_float('delta_for_early_stopping', 0.5, 'How much worst the results must get in order'\n",
    "                                                    ' for training to be terminated.'\n",
    "                                                    ' 0.05 mean 5% worst than best we had.')\n",
    "\n",
    "#                       Network Architecture Specific Flags\n",
    "# flags.DEFINE_integer('frame_size', 384, 'Dimensionality of the input for a single frame')\n",
    "flags.DEFINE_integer('frame_size', 204, 'Dimensionality of the input for a single frame')\n",
    "\n",
    "flags.DEFINE_integer(\"num_hidden_layers\", 1, \"Number of hidden layers\")\n",
    "flags.DEFINE_integer(\"middle_layer\", 1, \"Number of hidden layers\")\n",
    "\n",
    "# flags.DEFINE_integer('layer1_width', 312, 'Number of units in each hidden layer ')\n",
    "flags.DEFINE_integer('layer1_width', 180, 'Number of units in each hidden layer ')\n",
    "\n",
    "# flags.DEFINE_integer('layer2_width', 248, 'Number of units in each hidden layer ')\n",
    "# flags.DEFINE_integer('layer3_width', 312, 'Number of units in each hidden layer ')\n",
    "\n",
    "#                           Constants\n",
    "\n",
    "flags.DEFINE_integer('seed', 123456, 'Random seed')\n",
    "\n",
    "flags.DEFINE_string('summary_dir', home_out('summaries_exp'),\n",
    "                    'Directory to put the summary data')\n",
    "\n",
    "flags.DEFINE_string('chkpt_dir', home_out('chkpts_exp'),\n",
    "                    'Directory to put the model checkpoints')\n",
    "\n",
    "flags.DEFINE_string('results_file', home_out('results.txt'),\n",
    "                    'File to put the experimental results')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "every-entrepreneur",
   "metadata": {},
   "source": [
    "# motion_repr_learning/ae/train.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "wrong-affiliation",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "class DataInfo(object):\n",
    "    \"\"\"Information about the datasets\n",
    "\n",
    "     Will be passed to the network for creating corresponding variables in the graph\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, data_sigma, train_shape, eval_shape, max_val, mean_pose):\n",
    "        \"\"\"DataInfo initializer\n",
    "\n",
    "        Args:\n",
    "          data_sigma:   variance in the dataset\n",
    "          train_shape:  dimensionality of the train dataset\n",
    "          eval_shape:   dimensionality of the evaluation dataset\n",
    "        \"\"\"\n",
    "        self.data_sigma = data_sigma\n",
    "        self.train_shape = train_shape\n",
    "        self.eval_shape = eval_shape\n",
    "        self.max_val = max_val\n",
    "        self.mean_pose = mean_pose\n",
    "\n",
    "\n",
    "###############################################\n",
    "####                                    #######\n",
    "####              TRAIN                 #######\n",
    "####                                    #######\n",
    "###############################################\n",
    "\n",
    "\n",
    "def learning(data, data_info, just_restore=False):\n",
    "    \"\"\" Training of the network\n",
    "\n",
    "    Args:\n",
    "        data:           dataset to train on\n",
    "        data_info :     meta information about this dataset (such as variance, mean pose, etc.)\n",
    "                        it is an object from the class DataInfo (defined at the top of this file)\n",
    "        just_restore:   weather we are going to only restore the model from the checkpoint\n",
    "                        or are we going to train it as well\n",
    "\n",
    "    Returns:\n",
    "        nn:             Neural Network trained on a data provided\n",
    "    \"\"\"\n",
    "\n",
    "    test = False\n",
    "    debug = False\n",
    "\n",
    "    with tf.Graph().as_default():\n",
    "\n",
    "        tf.set_random_seed(FLAGS.seed)\n",
    "\n",
    "        start_time = time.time()\n",
    "\n",
    "        # Read the flags\n",
    "        variance = FLAGS.variance_of_noise\n",
    "        num_hidden = FLAGS.num_hidden_layers # 1\n",
    "        dropout = FLAGS.dropout\n",
    "        learning_rate = FLAGS.learning_rate\n",
    "        batch_size = FLAGS.batch_size\n",
    "\n",
    "        hidden_shapes = [FLAGS.layer1_width\n",
    "                         for j in range(num_hidden)]\n",
    "        print(\"hidden_shapes:\", hidden_shapes)\n",
    "        \n",
    "        # Check if the flags makes sence\n",
    "        if dropout < 0 or variance < 0:\n",
    "            print('ERROR! Have got negative values in the flags!')\n",
    "            exit(1)\n",
    "\n",
    "        # Allow TensorFlow to change device allocation when needed\n",
    "        config = tf.ConfigProto(allow_soft_placement=True)  # log_device_placement=True)\n",
    "        # Adjust configuration so that multiple executions are possible\n",
    "        config.gpu_options.allow_growth = True\n",
    "\n",
    "        # Start a session\n",
    "        sess = tf.Session(config=config)\n",
    "\n",
    "        if debug:\n",
    "            sess = tf_debug.TensorBoardDebugWrapperSession(sess, \"taras-All-Series:6064\")\n",
    "\n",
    "        # Create a neural network\n",
    "        shape = [FLAGS.frame_size * FLAGS.chunk_length] + hidden_shapes + [\n",
    "            FLAGS.frame_size * FLAGS.chunk_length]\n",
    "        print(\"FLAGS.frame_size:\", FLAGS.frame_size)\n",
    "        print(\"shape:\", shape)\n",
    "        nn = DAE(shape, sess, variance, data_info)\n",
    "        print('\\nDAE with the following shape was created : ', shape)\n",
    "\n",
    "        # Initialize input_producer\n",
    "        sess.run(tf.local_variables_initializer())\n",
    "\n",
    "        max_val = nn.max_val\n",
    "\n",
    "        with tf.variable_scope(\"Train\"):\n",
    "\n",
    "            ##############        DEFINE  Optimizer and training OPERATOR      ############\n",
    "\n",
    "            # Define the optimizer\n",
    "            optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "\n",
    "            # Do gradient clipping\n",
    "            tvars = tf.trainable_variables()\n",
    "            grads, _ = tf.clip_by_global_norm(tf.gradients(nn._loss, tvars), 1e12)\n",
    "            train_op = optimizer.apply_gradients(zip(grads, tvars),\n",
    "                                                 global_step=tf.train.get_or_create_global_step())\n",
    "\n",
    "            # Prepare for making a summary for TensorBoard\n",
    "            train_error = tf.placeholder(dtype=tf.float32, shape=(), name='train_error')\n",
    "            eval_error = tf.placeholder(dtype=tf.float32, shape=(), name='eval_error')\n",
    "\n",
    "            train_summary_op = tf.summary.scalar('Train_error', train_error)\n",
    "            eval_summary_op = tf.summary.scalar('Validation_error', eval_error)\n",
    "\n",
    "            summary_dir = FLAGS.summary_dir\n",
    "            summary_writer = tf.summary.FileWriter(summary_dir, graph=tf.get_default_graph())\n",
    "\n",
    "            num_batches = int(data.train.num_sequences / batch_size)\n",
    "\n",
    "            # Initialize the part of the graph with the input data\n",
    "            sess.run(nn._train_data.initializer,\n",
    "                     feed_dict={nn._train_data_initializer: data.train.sequences})\n",
    "            sess.run(nn._valid_data.initializer,\n",
    "                     feed_dict={nn._valid_data_initializer: data.test.sequences})\n",
    "\n",
    "            # Start input enqueue threads.\n",
    "            coord = tf.train.Coordinator()\n",
    "            threads = tf.train.start_queue_runners(sess=sess, coord=coord)\n",
    "\n",
    "            if FLAGS.pretrain:\n",
    "                layers_amount = len(nn.shape) - 2\n",
    "\n",
    "                # create an optimizers\n",
    "                pretrain_optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "\n",
    "                # Make an array of the trainers for all the layers\n",
    "                trainers = [pretrain_optimizer.minimize(\n",
    "                    loss_reconstruction(nn.run_less_layers(nn._input_, i+1),\n",
    "                                           nn.run_less_layers(nn._input_, i+1, is_target=True),\n",
    "                                           max_val, pretrain=True),\n",
    "                    global_step=tf.train.get_or_create_global_step(),\n",
    "                    name='Layer_wise_optimizer_'+str(i))\n",
    "                            for i in range(layers_amount)]\n",
    "\n",
    "                # Initialize all the variables\n",
    "                sess.run(tf.global_variables_initializer())\n",
    "\n",
    "            else:\n",
    "                print(\"Initializing variables ...\\n\")\n",
    "                sess.run(tf.global_variables_initializer())\n",
    "\n",
    "            # Create a saver\n",
    "            saver = tf.train.Saver(write_version=tf.train.SaverDef.V2)\n",
    "            chkpt_file = FLAGS.chkpt_dir + '/chkpt-final'\n",
    "\n",
    "            # restore model, if needed\n",
    "            if FLAGS.restore:\n",
    "                saver.restore(sess, chkpt_file)\n",
    "                print(\"Model restored from the file \" + str(chkpt_file) + '.')\n",
    "\n",
    "            if just_restore:\n",
    "                coord.request_stop()\n",
    "                return nn\n",
    "\n",
    "            # A few initialization for the early stopping\n",
    "            delta = FLAGS.delta_for_early_stopping  # error tolerance for early stopping\n",
    "            best_error = 10000\n",
    "            num_valid_batches = int(data.test.num_sequences / batch_size)\n",
    "\n",
    "            try:  # running enqueue threads.\n",
    "\n",
    "                # Pretrain\n",
    "                if FLAGS.pretrain:\n",
    "                    layerwise_pretrain(nn, trainers, layers_amount, num_batches)\n",
    "\n",
    "                # Train the whole network jointly\n",
    "                step = 0\n",
    "                print('\\nFinetune the whole network on ', num_batches, ' batches with ', batch_size,\n",
    "                      ' training examples in each for', FLAGS.training_epochs, ' epochs...')\n",
    "                print(\"\")\n",
    "                print(\" ______________ ______\")\n",
    "                print(\"|     Epoch    | RMSE |\")\n",
    "                print(\"|------------  |------|\")\n",
    "\n",
    "                while not coord.should_stop():\n",
    "                    _, train_error_ = sess.run([train_op, nn._reconstruction_loss], feed_dict={})\n",
    "\n",
    "                    if step % num_batches == 0:\n",
    "                        epoch = step * 1.0 / num_batches\n",
    "\n",
    "                        train_summary = sess.run(train_summary_op, feed_dict={\n",
    "                            train_error: np.sqrt(train_error_)})\n",
    "\n",
    "                        # Print results of screen\n",
    "                        epoch_str = \"| {0:3.0f} \".format(epoch)[:5]\n",
    "                        perc_str = \"({0:3.2f}\".format(epoch*100.0 / FLAGS.training_epochs)[:5]\n",
    "                        error_str = \"%) |{0:5.2f}\".format(train_error_)[:10] + \"|\"\n",
    "                        print(epoch_str, perc_str, error_str)\n",
    "\n",
    "                        if epoch % 5 == 0 and test:\n",
    "\n",
    "                            rmse = test(nn, FLAGS.data_dir + '/test_1.binary')\n",
    "                            print(\"\\nOur RMSE for the first test sequence is : \", rmse)\n",
    "\n",
    "                            rmse = test(nn, FLAGS.data_dir + '/test_2.binary')\n",
    "                            print(\"\\nOur RMSE for the second test sequenceis : \", rmse)\n",
    "\n",
    "                        if epoch > 0:\n",
    "                            summary_writer.add_summary(train_summary, step)\n",
    "\n",
    "                            # Evaluate on the validation sequences\n",
    "                            error_sum = 0\n",
    "                            for valid_batch in range(num_valid_batches):\n",
    "                                curr_err = sess.run([nn._valid_loss], feed_dict={})\n",
    "                                error_sum += curr_err[0]\n",
    "                            new_error = error_sum / (num_valid_batches)\n",
    "                            eval_sum = sess.run(eval_summary_op,\n",
    "                                                feed_dict={eval_error: np.sqrt(new_error)})\n",
    "                            summary_writer.add_summary(eval_sum, step)\n",
    "\n",
    "                            # Early stopping\n",
    "                            if FLAGS.early_stopping:\n",
    "                                if (new_error - best_error) / best_error > delta:\n",
    "                                    print('After ' + str(step) + ' steps started overfitting')\n",
    "                                    break\n",
    "                                if new_error < best_error:\n",
    "                                    best_error = new_error\n",
    "\n",
    "                                    # Saver for the model\n",
    "                                    save_path = saver.save(sess, chkpt_file)\n",
    "\n",
    "                            if epoch % 5 == 0:\n",
    "                                # Save for the model\n",
    "                                save_path = saver.save(sess, chkpt_file)\n",
    "                                print('Done training for %d epochs' % (epoch))\n",
    "                                print(\"The model was saved in file: %s\" % save_path)\n",
    "\n",
    "                    step += 1\n",
    "\n",
    "            except tf.errors.OutOfRangeError:\n",
    "                if not FLAGS.early_stopping:\n",
    "                    # Save the model\n",
    "                    save_path = saver.save(sess, chkpt_file)\n",
    "                print('Done training for %d epochs, %d steps.' % (FLAGS.training_epochs, step))\n",
    "                print(\"The final model was saved in file: %s\" % save_path)\n",
    "            finally:\n",
    "                # When done, ask the threads to stop.\n",
    "                coord.request_stop()\n",
    "\n",
    "            # Wait for threads to finish.\n",
    "            coord.join(threads)\n",
    "\n",
    "        duration = (time.time() - start_time) / 60  # in minutes, instead of seconds\n",
    "\n",
    "        print(\"The training was running for %.3f  min\" % (duration))\n",
    "\n",
    "        return nn\n",
    "\n",
    "def decode(nn, represent_vec):\n",
    "    \"\"\" Decoding a representation from AE (AutoEncoder)\n",
    "\n",
    "      Args:\n",
    "          nn:              trained AutoEncoder\n",
    "          represent_vec:   input sequence to be encoded\n",
    "\n",
    "      Returns:\n",
    "          output_seq:  vector of encoding\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"Decoding ...\")\n",
    "\n",
    "    with nn.session.graph.as_default() as sess:\n",
    "\n",
    "        # Obtain important constants\n",
    "\n",
    "        sess = nn.session\n",
    "        mean_pose = nn.mean_pose\n",
    "        max_val = nn.max_val\n",
    "\n",
    "        #                    GET THE DATA\n",
    "\n",
    "        # Check if we can cut sequence into the chunks of length ae.sequence_length\n",
    "        if represent_vec.shape[0] < nn.sequence_length:\n",
    "            mupliplication_factor = int(nn.batch_size * nn.sequence_length /\n",
    "                                        represent_vec.shape[0]) + 1\n",
    "\n",
    "            # Pad the sequence with itself in order to fill the sequence completely\n",
    "            represent_vec = np.tile(represent_vec, (mupliplication_factor, 1))\n",
    "            print(\"Test sequence was way too short, so we padded it with itself!\")\n",
    "\n",
    "        # Split it into chunks\n",
    "        all_chunks = represent_vec\n",
    "\n",
    "        if all_chunks.shape[0] < nn.batch_size:\n",
    "            mupliplication_factor = int(nn.batch_size / all_chunks.shape[0]) + 1\n",
    "\n",
    "            # Pad the sequence with itself in order to fill the batch completely\n",
    "            all_chunks = np.tile(all_chunks, (mupliplication_factor, 1))\n",
    "\n",
    "        # Batch those chunks\n",
    "        batches = np.array([all_chunks[i:i + nn.batch_size, :]\n",
    "                            for i in range(0, len(all_chunks) - nn.batch_size + 1, nn.batch_size)])\n",
    "\n",
    "        numb_of_batches = batches.shape[0]\n",
    "\n",
    "        #                    RUN THE NETWORK\n",
    "\n",
    "        output_batches = np.array([])\n",
    "\n",
    "        # Go over all batches one by one\n",
    "        for batch_numb in range(numb_of_batches):\n",
    "            output_batch = sess.run([nn._decode],\n",
    "                                    feed_dict={nn._representation: batches[batch_numb]})\n",
    "            output_batches = np.append(output_batches, output_batch, axis=0) \\\n",
    "                if output_batches.size else np.array(output_batch)\n",
    "\n",
    "        # Postprocess...\n",
    "        output_vec = np.reshape(output_batches, (-1, FLAGS.chunk_length * FLAGS.frame_size))\n",
    "\n",
    "        # Convert back to original values\n",
    "        reconstructed = convert_back_to_3D(output_vec, max_val, mean_pose)\n",
    "        print(\"reconstructed:\", reconstructed)\n",
    "        return reconstructed\n",
    "\n",
    "###############################################\n",
    "####                                    #######\n",
    "####      LAYERWISE PRETRAINING         #######\n",
    "####                                    #######\n",
    "###############################################"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "norwegian-climb",
   "metadata": {},
   "source": [
    "# motion_repr_learning/ae/DAE.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "devoted-center",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_noise(x, variance_multiplier, sigma):\n",
    "    \"\"\"\n",
    "           Add Gaussian noise to the data\n",
    "           Args:\n",
    "               x                   - input vector\n",
    "               variance_multiplier - coefficient to multiple variance of the noise on\n",
    "               sigma               - variance of the dataset\n",
    "           Returns:\n",
    "               x - output vector, noisy data\n",
    "    \"\"\"\n",
    "    eps = 1e-15\n",
    "    noise = tf.random_normal(x.shape, 0.0, stddev=np.multiply(sigma, variance_multiplier) + eps)\n",
    "    x = x + noise\n",
    "    return x\n",
    "\n",
    "def loss_reconstruction(output, target, max_vals, pretrain=False):\n",
    "    \"\"\" Reconstruction error. Square of the RMSE\n",
    "\n",
    "    Args:\n",
    "      output:    tensor of net output\n",
    "      target:    tensor of net we are trying to reconstruct\n",
    "      max_vals:  array of absolute maximal values in the dataset,\n",
    "                is used for scaling an error to the original space\n",
    "      pretrain:  wether we are using it during the pretraining phase\n",
    "    Returns:\n",
    "      Scalar tensor of mean squared Eucledean distance\n",
    "    \"\"\"\n",
    "    with tf.name_scope(\"reconstruction_loss\"):\n",
    "        net_output_tf = tf.convert_to_tensor(tf.cast(output, tf.float32), name='input')\n",
    "        target_tf = tf.convert_to_tensor(tf.cast(target, tf.float32), name='target')\n",
    "\n",
    "        # Euclidean distance between net_output_tf,target_tf\n",
    "        error = tf.subtract(net_output_tf, target_tf)\n",
    "\n",
    "        if not pretrain:\n",
    "            # Convert it back from the [-1,1] to original values\n",
    "            error_scaled = tf.multiply(error, max_vals[np.newaxis, :] + 1e-15)\n",
    "        else:\n",
    "            error_scaled = error\n",
    "\n",
    "        squared_error = tf.reduce_mean(tf.square(error_scaled, name=\"square\"), name=\"averaging\")\n",
    "    return squared_error\n",
    "\n",
    "class DAE:\n",
    "    \"\"\" Denoising Autoendoder (DAE)\n",
    "\n",
    "    More details about the network in the original paper:\n",
    "    http://www.jmlr.org/papers/v11/vincent10a.html\n",
    "\n",
    "    The user specifies the structure of this network\n",
    "    by specifying number of inputs, the number of hidden\n",
    "    units for each layer and the number of final outputs.\n",
    "    All this information is set in the utils/flags.py file.\n",
    "\n",
    "    The number of input neurons is defined as a frame_size*chunk_length,\n",
    "    since it will take a time-window as an input\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, shape, sess, variance_coef, data_info):\n",
    "        \"\"\"DAE initializer\n",
    "\n",
    "        Args:\n",
    "          shape:          list of ints specifying\n",
    "                          num input, hidden1 units,...hidden_n units, num outputs\n",
    "          sess:           tensorflow session object to use\n",
    "          varience_coef:  multiplicative factor for the variance of noise wrt the variance of data\n",
    "          data_info:      key information about the dataset\n",
    "        \"\"\"\n",
    "\n",
    "        self.__shape = shape  # [input_dim,hidden1_dim,...,hidden_n_dim,output_dim]\n",
    "        self.__variables = {}\n",
    "        self.__sess = sess\n",
    "\n",
    "        self.num_hidden_layers = np.size(shape) - 2\n",
    "#         print('self.num_hidden_layers:',self.num_hidden_layers)\n",
    "        self.batch_size = FLAGS.batch_size\n",
    "        self.sequence_length = FLAGS.chunk_length\n",
    "\n",
    "        self.scaling_factor = 1\n",
    "\n",
    "\t    # maximal value and mean pose in the dataset (used for scaling it to interval [-1,1] and back)\n",
    "        self.max_val = data_info.max_val\n",
    "        self.mean_pose = data_info.mean_pose\n",
    "\n",
    "\n",
    "        #################### Add the DATASETS to the GRAPH ###############\n",
    "\n",
    "        #### 1 - TRAIN ###\n",
    "        self._train_data_initializer = tf.placeholder(dtype=tf.float32,\n",
    "                                                      shape=data_info.train_shape)\n",
    "        self._train_data = tf.Variable(self._train_data_initializer,\n",
    "                                       trainable=False, collections=[], name='Train_data')\n",
    "        train_epochs = FLAGS.training_epochs + FLAGS.pretraining_epochs * FLAGS.num_hidden_layers\n",
    "        train_frames = tf.train.slice_input_producer([self._train_data], num_epochs=train_epochs)\n",
    "        self._train_batch = tf.train.shuffle_batch(train_frames,\n",
    "                                                   batch_size=FLAGS.batch_size, capacity=5000,\n",
    "                                                   min_after_dequeue=1000, name='Train_batch')\n",
    "        print(\"data_info.train_shape:\", data_info.train_shape)\n",
    "        print(\"self._train_data:\", self._train_data)\n",
    "        print(\"train_epochs:\", train_epochs)\n",
    "        print(\"train_frames:\", train_frames)\n",
    "        \n",
    "        #### 2 - VALIDATE, can be used as TEST ###\n",
    "        # When optimizing - this dataset stores as a validation dataset,\n",
    "        # when testing - this dataset stores a test dataset\n",
    "        self._valid_data_initializer = tf.placeholder(dtype=tf.float32,\n",
    "                                                      shape=data_info.eval_shape)\n",
    "        self._valid_data = tf.Variable(self._valid_data_initializer,\n",
    "                                       trainable=False, collections=[], name='Valid_data')\n",
    "        valid_frames = tf.train.slice_input_producer([self._valid_data],\n",
    "                                                     num_epochs=FLAGS.training_epochs)\n",
    "        self._valid_batch = tf.train.shuffle_batch(valid_frames,\n",
    "                                                   batch_size=FLAGS.batch_size, capacity=5000,\n",
    "                                                   min_after_dequeue=1000, name='Valid_batch')\n",
    "\n",
    "        if FLAGS.weight_decay is not None:\n",
    "            print('\\nWe apply weight decay')\n",
    "\n",
    "        ### Specify tensorflow setup  ###\n",
    "        with sess.graph.as_default():\n",
    "\n",
    "            ##############        SETUP VARIABLES       ######################\n",
    "\n",
    "            with tf.variable_scope(\"AE_Variables\"):\n",
    "\n",
    "                for i in range(self.num_hidden_layers + 1):  # go over layers\n",
    "\n",
    "                    # create variables for matrices and biases for each layer\n",
    "                    self._create_variables(i, FLAGS.weight_decay)\n",
    "\n",
    "                ##############        DEFINE THE NETWORK     ##################\n",
    "\n",
    "                ''' 1 - Setup network for TRAINing '''\n",
    "                # Input noisy data and reconstruct the original one\n",
    "                # as in Denoising AutoEncoder                \n",
    "                self._input_ = add_noise(self._train_batch, variance_coef, data_info.data_sigma)\n",
    "                self._target_ = self._train_batch\n",
    "                # Define output and loss for the training data\n",
    "                self._output, _, _ = self.construct_graph(self._input_, FLAGS.dropout)\n",
    "                self._reconstruction_loss = loss_reconstruction(self._output,\n",
    "                                                                self._target_, self.max_val)\n",
    "                tf.add_to_collection('losses', self._reconstruction_loss)  # add weight decay loses\n",
    "                self._loss = tf.add_n(tf.get_collection('losses'), name='total_loss')\n",
    "\n",
    "                ''' 2 - Setup network for TESTing '''\n",
    "                self._valid_input_ = self._valid_batch\n",
    "                self._valid_target_ = self._valid_batch\n",
    "\n",
    "                # Define output (no dropout)\n",
    "                self._valid_output, self._encode, self._decode = \\\n",
    "                    self.construct_graph(self._valid_input_, 1)\n",
    "\n",
    "                # Define loss\n",
    "                self._valid_loss = loss_reconstruction(self._valid_output,\n",
    "                                                       self._valid_target_, self.max_val)\n",
    "    @property\n",
    "    def session(self):\n",
    "        \"\"\" Interface for the session\"\"\"\n",
    "        return self.__sess\n",
    "\n",
    "    @property\n",
    "    def shape(self):\n",
    "        \"\"\" Interface for the shape\"\"\"\n",
    "        return self.__shape\n",
    "\n",
    "    # Make more comfortable interface to the network weights\n",
    "\n",
    "    def _w(self, n, suffix=\"\"):\n",
    "        return self[\"matrix\"+str(n)+suffix]\n",
    "\n",
    "    def _b(self, n, suffix=\"\"):\n",
    "        return self[\"bias\"+str(n)+suffix]\n",
    "\n",
    "    @staticmethod\n",
    "    def _feedforward(x, w, b):\n",
    "        \"\"\"\n",
    "        Traditional feedforward layer: multiply on weight matrix, add bias vector\n",
    "         and apply activation function\n",
    "\n",
    "        Args:\n",
    "            x: input ( usually - batch of vectors)\n",
    "            w: matrix to be multiplied on\n",
    "            b: bias to be added\n",
    "\n",
    "        Returns:\n",
    "            y: result of applying this feedforward layer\n",
    "        \"\"\"\n",
    "\n",
    "        y = tf.tanh(tf.nn.bias_add(tf.matmul(x, w), b))\n",
    "        return y\n",
    "\n",
    "    def construct_graph(self, input_seq_pl, dropout):\n",
    "\n",
    "        \"\"\" Construct a TensorFlow graph for the AutoEncoding network\n",
    "\n",
    "        Args:\n",
    "          input_seq_pl:     tf placeholder for input data: size [batch_size, sequence_length * DoF]\n",
    "          dropout:          how much of the input neurons will be activated, value in range [0,1]\n",
    "        Returns:\n",
    "          output:           output tensor: result of running input placeholder through the network\n",
    "          middle_layer:     tensor which is encoding input placeholder into a representation\n",
    "          decoding:         tensor which is decoding a representation back into the input vector\n",
    "        \"\"\"\n",
    "\n",
    "        network_input = input_seq_pl\n",
    "        print(\"network_input:\",network_input)\n",
    "        print(\"FLAGS.frame_size:\", FLAGS.frame_size)\n",
    "        curr_layer = tf.reshape(network_input, [self.batch_size,\n",
    "                                                FLAGS.chunk_length * FLAGS.frame_size])\n",
    "\n",
    "        numb_layers = self.num_hidden_layers + 1\n",
    "\n",
    "        with tf.name_scope(\"Joint_run\"):\n",
    "\n",
    "            # Pass through the network\n",
    "            for i in range(numb_layers):\n",
    "\n",
    "                if i == FLAGS.middle_layer:\n",
    "                    # Save middle layer\n",
    "                    with tf.name_scope('middle_layer'):\n",
    "                        middle_layer = tf.identity(curr_layer)\n",
    "\n",
    "                with tf.name_scope('hidden'+str(i)):\n",
    "\n",
    "                    # First - Apply Dropout\n",
    "                    curr_layer = tf.nn.dropout(curr_layer, dropout)\n",
    "\n",
    "                    w = self._w(i + 1)\n",
    "                    b = self._b(i + 1)\n",
    "\n",
    "                    curr_layer = self._feedforward(curr_layer, w, b)\n",
    "\n",
    "            output = curr_layer\n",
    "\n",
    "        # Now create a decoding network\n",
    "\n",
    "        with tf.name_scope(\"Decoding\"):\n",
    "\n",
    "            layer = self._representation = tf.placeholder\\\n",
    "                (dtype=tf.float32, shape=middle_layer.get_shape().as_list(), name=\"Respres.\")\n",
    "\n",
    "            for i in range(FLAGS.middle_layer, numb_layers):\n",
    "\n",
    "                with tf.name_scope('hidden' + str(i)):\n",
    "\n",
    "                    # First - Apply Dropout\n",
    "                    layer = tf.nn.dropout(layer, dropout)\n",
    "\n",
    "                    w = self._w(i + 1)\n",
    "                    b = self._b(i + 1)\n",
    "\n",
    "                    layer = self._feedforward(layer, w, b)\n",
    "\n",
    "            decoding = layer\n",
    "\n",
    "        return output, middle_layer, decoding\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        \"\"\"Get AutoEncoder tf variable\n",
    "\n",
    "        Returns the specified variable created by this object.\n",
    "        Names are weights#, biases#, biases#_out, weights#_fixed,\n",
    "        biases#_fixed.\n",
    "\n",
    "        Args:\n",
    "         item: string, variables internal name\n",
    "        Returns:\n",
    "         Tensorflow variable\n",
    "        \"\"\"\n",
    "        return self.__variables[item]\n",
    "\n",
    "    def __setitem__(self, key, value):\n",
    "        \"\"\"Store a TensorFlow variable\n",
    "\n",
    "        NOTE: Don't call this explicitly. It should\n",
    "        be used only internally when setting up\n",
    "        variables.\n",
    "\n",
    "        Args:\n",
    "          key: string, name of variable\n",
    "          value: tensorflow variable\n",
    "        \"\"\"\n",
    "        self.__variables[key] = value\n",
    "\n",
    "    def _create_variables(self, i, wd):\n",
    "        \"\"\"Helper to create an initialized Variable with weight decay.\n",
    "        Note that the Variable is initialized with a truncated normal distribution.\n",
    "        A weight decay is added only if 'wd' is specified.\n",
    "        If 'wd' is None, weight decay is not added for this Variable.\n",
    "\n",
    "        This function was taken from the web\n",
    "\n",
    "        Args:\n",
    "          i: number of hidden layer\n",
    "          wd: add L2Loss weight decay multiplied by this float.\n",
    "        Returns:\n",
    "          Nothing\n",
    "        \"\"\"\n",
    "\n",
    "        # Initialize Train weights\n",
    "        w_shape = (self.__shape[i], self.__shape[i + 1])\n",
    "        a = tf.multiply(2.0, tf.sqrt(6.0 / (w_shape[0] + w_shape[1])))\n",
    "        name_w = \"matrix\"+str(i + 1)\n",
    "        self[name_w] = tf.get_variable(\"Variables/\"+name_w,\n",
    "                                       initializer=tf.random_uniform(w_shape, -1 * a, a))\n",
    "\n",
    "        # Add weight to the loss function for weight decay\n",
    "        if wd is not None:\n",
    "            weight_decay = tf.multiply(tf.nn.l2_loss(self[name_w]), wd, name='wgt_'+str(i)+'_loss')\n",
    "            tf.add_to_collection('losses', weight_decay)\n",
    "\n",
    "        # Add the histogram summary\n",
    "        tf.summary.histogram(name_w, self[name_w])\n",
    "\n",
    "        # Initialize Train biases\n",
    "        name_b = \"bias\"+str(i + 1)\n",
    "        b_shape = (self.__shape[i + 1],)\n",
    "        self[name_b] = tf.get_variable(\"Variables/\"+name_b, initializer=tf.zeros(b_shape))\n",
    "\n",
    "        if i < self.num_hidden_layers:\n",
    "            # Hidden layer pretrained weights\n",
    "            # which are used after pretraining before fine-tuning\n",
    "            self[name_w + \"_pretr\"] = tf.get_variable(name=\"Var/\" + name_w + \"_pretr\", initializer=\n",
    "                                                      tf.random_uniform(w_shape, -1 * a, a),\n",
    "                                                      trainable=False)\n",
    "            # Hidden layer pretrained biases\n",
    "            self[name_b + \"_pretr\"] = tf.get_variable(\"Var/\"+name_b+\"_pretr\", trainable=False,\n",
    "                                                      initializer=tf.zeros(b_shape))\n",
    "\n",
    "            # Pretraining output training biases\n",
    "            name_b_out = \"bias\" + str(i+1) + \"_out\"\n",
    "            b_shape = (self.__shape[i],)\n",
    "            b_init = tf.zeros(b_shape)\n",
    "            self[name_b_out] = tf.get_variable(name=\"Var/\"+name_b_out, initializer=b_init,\n",
    "                                               trainable=True)\n",
    "\n",
    "    def run_less_layers(self, input_pl, n, is_target=False):\n",
    "        \"\"\"Return result of a net after n layers or n-1 layer (if is_target is true)\n",
    "           This function will be used for the layer-wise pretraining of the AE\n",
    "        Args:\n",
    "          input_pl:  TensorFlow placeholder of AE inputs\n",
    "          n:         int specifying pretrain step\n",
    "          is_target: bool specifying if required tensor\n",
    "                      should be the target tensor\n",
    "                     meaning if we should run n layers or n-1 (if is_target)\n",
    "        Returns:\n",
    "          Tensor giving pretraining net result or pretraining target\n",
    "        \"\"\"\n",
    "        assert n > 0\n",
    "        assert n <= self.num_hidden_layers\n",
    "\n",
    "        last_output = input_pl\n",
    "\n",
    "        for i in range(n - 1):\n",
    "            w = self._w(i + 1, \"_pretrained\")\n",
    "            b = self._b(i + 1, \"_pretrained\")\n",
    "\n",
    "            last_output = self._feedforward(last_output, w, b)\n",
    "\n",
    "        if is_target:\n",
    "            return last_output\n",
    "\n",
    "        last_output = self._feedforward(last_output, self._w(n), self._b(n))\n",
    "\n",
    "        out = self._feedforward(last_output, self._w(n), self[\"bias\" + str(n) + \"_out\"])\n",
    "\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "necessary-combine",
   "metadata": {},
   "source": [
    "# motion_repr_learning/ae/utils/utils.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ethical-taiwan",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Dataset class\"\"\"\n",
    "\n",
    "class DataSet(object):\n",
    "    '''\n",
    "    A class for storing a dataset and all important information,\n",
    "    which might be needed during training,\n",
    "    such as batch size amount of epochs completed and so on.\n",
    "    '''\n",
    "\n",
    "\n",
    "    def __init__(self, sequences, batch_size):\n",
    "        self._batch_size = batch_size\n",
    "        self._sequences = sequences  # all the sequnces in the dataset\n",
    "        self._num_sequences = sequences.shape[0]\n",
    "        self._epochs_completed = 0\n",
    "        self._index_in_epoch = 0\n",
    "\n",
    "    # Make interface to the protected variables\n",
    "    @property\n",
    "    def sequences(self):\n",
    "        return self._sequences\n",
    "\n",
    "    @property\n",
    "    def num_sequences(self):\n",
    "        return self._num_sequences\n",
    "\n",
    "class DataSets(object):\n",
    "    '''\n",
    "      A class for storing Train and Eval datasets and all related information,\n",
    "      '''\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "subject-workshop",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_motion_data(data_dir): #preprocessing\n",
    "    \"\"\"\n",
    "    Read and preprocess the motion dataset\n",
    "\n",
    "    Args:\n",
    "        data_dir:           a directory with the dataset\n",
    "    Return:\n",
    "        Y_train:            an array of the training dataset\n",
    "        Y_train_normalized: training dataset normalized to the values [-1,1]\n",
    "        Y_test:             an array of the test dataset\n",
    "        Y_test_normalized:  test dataset normalized to the values [-1,1]\n",
    "        Y_dev_normalized:   dev dataset normalized to the values [-1,1]\n",
    "        max_val:            maximal values in the dataset\n",
    "        mean_pose:          mean pose of the dataset\n",
    "    \"\"\"\n",
    "\n",
    "    # Get the data\n",
    "\n",
    "    Y_train = np.load(data_dir + '/Y_train.npy')\n",
    "    Y_dev = np.load(data_dir + '/Y_dev.npy')\n",
    "    Y_test = np.load(data_dir + '/Y_test.npy')\n",
    "\n",
    "    # Normalize dataset\n",
    "    max_val = np.amax(np.absolute(Y_train), axis=(0))\n",
    "    mean_pose = Y_train.mean(axis=(0))\n",
    "\n",
    "    Y_train_centered = Y_train - mean_pose[np.newaxis, :]\n",
    "    Y_dev_centered = Y_dev - mean_pose[np.newaxis, :]\n",
    "    Y_test_centered = Y_test - mean_pose[np.newaxis, :]\n",
    "\n",
    "    # Scales all values in the input_data to be between -1 and 1\n",
    "    eps = 1e-8\n",
    "    Y_train_normalized = np.divide(Y_train_centered, max_val[np.newaxis, :] + eps)\n",
    "    Y_dev_normalized = np.divide(Y_dev_centered, max_val[np.newaxis, :] + eps)\n",
    "    Y_test_normalized = np.divide(Y_test_centered, max_val[np.newaxis, :] + eps)\n",
    "\n",
    "    # Reshape to accomodate multiple frames at each input\n",
    "\n",
    "    if FLAGS.chunk_length > 1:\n",
    "        Y_train_normalized = reshape_dataset(Y_train_normalized)\n",
    "        Y_dev_normalized = reshape_dataset(Y_dev_normalized)\n",
    "        Y_test_normalized = reshape_dataset(Y_test_normalized)\n",
    "\n",
    "    # Pad max values and the mean pose, if neeeded\n",
    "    if FLAGS.chunk_length > 1:\n",
    "        max_val = np.tile(max_val, FLAGS.chunk_length)\n",
    "        mean_pose = np.tile(mean_pose, FLAGS.chunk_length)\n",
    "\n",
    "\n",
    "#     Some tests for flags\n",
    "    if FLAGS.restore and FLAGS.pretrain:\n",
    "        print('ERROR! You cannot restore and pretrain at the same time!'\n",
    "              ' Please, chose one of these options')\n",
    "        exit(1)\n",
    "\n",
    "    if FLAGS.middle_layer > FLAGS.num_hidden_layers:\n",
    "        print('ERROR! Middle layer cannot be more than number of hidden layers!'\n",
    "              ' Please, update flags')\n",
    "        exit(1)\n",
    "\n",
    "#     Y_dev_normalized = 0\n",
    "#     Y_test_normalized = 0\n",
    "#     Y_test = 0\n",
    "    return Y_train_normalized, Y_train, Y_test_normalized, Y_test,\\\n",
    "           Y_dev_normalized, max_val, mean_pose\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "unique-venice",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_nn(train_data, dev_data, max_val, mean_pose, restoring):\n",
    "    \"\"\"\n",
    "    Train or restore a neural network\n",
    "    Args:\n",
    "     train_data:         training dataset normalized to the values [-1,1]\n",
    "     dev_data:           dev dataset normalized to the values [-1,1]\n",
    "     max_val:            maximal values in the dataset\n",
    "     mean_pose:          mean pose of the dataset\n",
    "     restoring:          weather  we are going to just restore already trained model\n",
    "    Returns:\n",
    "     nn: neural network, which is ready to use\n",
    "    \"\"\"\n",
    "\n",
    "    # Create DataSet object\n",
    "\n",
    "    data = DataSets()\n",
    "\n",
    "    data.train = DataSet(train_data, FLAGS.batch_size)\n",
    "    data.test = DataSet(dev_data, FLAGS.batch_size)\n",
    "\n",
    "    # Assign variance\n",
    "    data.train.sigma = np.std(train_data, axis=(0, 1))\n",
    "\n",
    "    # Create information about the dataset\n",
    "    data_info = DataInfo(data.train.sigma, data.train._sequences.shape,\n",
    "                            data.test._sequences.shape, max_val, mean_pose)\n",
    "\n",
    "\n",
    "    # Set \"restore\" flag\n",
    "    FLAGS.restore = restoring\n",
    "    \n",
    "#     self.data_sigma = data_sigma\n",
    "#     self.train_shape = train_shape\n",
    "#     self.eval_shape = eval_shape\n",
    "#     self.max_val = max_val\n",
    "#     self.mean_pose = mean_pose\n",
    "\n",
    "    \n",
    "#     print(\"data.train:\", data.train)\n",
    "#     print(\"data_info.data_sigma:\", data_info.data_sigma)\n",
    "#     print(\"data_info.train_shape:\", data_info.train_shape)\n",
    "#     print(\"data_info.eval_shape:\", data_info.eval_shape)\n",
    "    \n",
    "    # Train the network\n",
    "    nn = learning(data, data_info, just_restore=restoring)\n",
    "\n",
    "    return nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "rising-cursor",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_back_to_3D(sequence, max_val, mean_pose):\n",
    "    '''\n",
    "    Convert back from the normalized values between -1 and 1 to original 3d coordinates\n",
    "    and unroll them into the sequence\n",
    "\n",
    "    Args:\n",
    "        sequence: sequence of the normalized values\n",
    "        max_val: maximal value in the dataset\n",
    "        mean_pose: mean value in the dataset\n",
    "\n",
    "    Return:\n",
    "        3d coordinates corresponding to the batch\n",
    "    '''\n",
    "\n",
    "    # Convert it back from the [-1,1] to original values\n",
    "#     reconstructed = np.multiply(sequence, max_val[np.newaxis, :] + 1e-15)\n",
    "    reconstructed = np.multiply(sequence, max_val[np.newaxis, :] + 1e-8)\n",
    "\n",
    "    # Add the mean pose back\n",
    "    reconstructed = reconstructed + mean_pose[np.newaxis, :]\n",
    "\n",
    "    # Unroll batches into the sequence\n",
    "    reconstructed = reconstructed.reshape(-1, reconstructed.shape[-1])\n",
    "    return reconstructed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "committed-tuition",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_reconstruction(output, target, max_vals, pretrain=False):\n",
    "    \"\"\" Reconstruction error. Square of the RMSE\n",
    "\n",
    "    Args:\n",
    "      output:    tensor of net output\n",
    "      target:    tensor of net we are trying to reconstruct\n",
    "      max_vals:  array of absolute maximal values in the dataset,\n",
    "                is used for scaling an error to the original space\n",
    "      pretrain:  wether we are using it during the pretraining phase\n",
    "    Returns:\n",
    "      Scalar tensor of mean squared Eucledean distance\n",
    "    \"\"\"\n",
    "    with tf.name_scope(\"reconstruction_loss\"):\n",
    "        net_output_tf = tf.convert_to_tensor(tf.cast(output, tf.float32), name='input')\n",
    "        target_tf = tf.convert_to_tensor(tf.cast(target, tf.float32), name='target')\n",
    "\n",
    "        # Euclidean distance between net_output_tf,target_tf\n",
    "        error = tf.subtract(net_output_tf, target_tf)\n",
    "\n",
    "        if not pretrain:\n",
    "            # Convert it back from the [-1,1] to original values\n",
    "            error_scaled = tf.multiply(error, max_vals[np.newaxis, :] + 1e-15)\n",
    "        else:\n",
    "            error_scaled = error\n",
    "\n",
    "        squared_error = tf.reduce_mean(tf.square(error_scaled, name=\"square\"), name=\"averaging\")\n",
    "    return squared_error\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "stable-alarm",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "starting-maintenance",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from tensorflow.keras.models import load_model\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "smoking-alarm",
   "metadata": {},
   "source": [
    "# predict.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "critical-mortality",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model_name, input_file, output_file):\n",
    "    \"\"\" Predict human gesture based on the speech\n",
    "\n",
    "    Args:\n",
    "        model_name:  name of the Keras model to be used\n",
    "        input_file:  file name of the audio input\n",
    "        output_file: file name for the gesture output\n",
    "\n",
    "    Returns:\n",
    "\n",
    "    \"\"\"\n",
    "    model = load_model(model_name)\n",
    "    X = np.load(input_file)\n",
    "\n",
    "    predicted = np.array(model.predict(X))\n",
    "    print(predicted.shape)\n",
    "    np.savetxt(output_file, predicted)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dried-thing",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Wen\\miniconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow_core\\python\\ops\\init_ops.py:97: calling GlorotUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From C:\\Users\\Wen\\miniconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow_core\\python\\ops\\init_ops.py:97: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From C:\\Users\\Wen\\miniconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow_core\\python\\ops\\resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "WARNING:tensorflow:From C:\\Users\\Wen\\miniconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow_core\\python\\ops\\init_ops.py:97: calling Ones.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From C:\\Users\\Wen\\miniconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow_core\\python\\ops\\init_ops.py:97: calling Orthogonal.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From C:\\Users\\Wen\\miniconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow_core\\python\\ops\\math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "(1218, 180)\n"
     ]
    }
   ],
   "source": [
    "model_name = 'first_model_3d_180_dim'\n",
    "input_file = 'data_3d_cord/test_inputs/X_test_audio42.npy'\n",
    "output_file = 'data_3d_cord/test_inputs/predict_audio42_20fps_3d_cord.txt'\n",
    "predict(model_name, input_file, output_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vietnamese-runner",
   "metadata": {},
   "source": [
    "# motion_repr_learning/ae/decode.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "frozen-daily",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hidden_shapes: [180]\n",
      "FLAGS.frame_size: 204\n",
      "shape: [204, 180, 204]\n",
      "WARNING:tensorflow:From <ipython-input-3-4f0f08365d52>:95: slice_input_producer (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.from_tensor_slices(tuple(tensor_list)).shuffle(tf.shape(input_tensor, out_type=tf.int64)[0]).repeat(num_epochs)`. If `shuffle=False`, omit the `.shuffle(...)`.\n",
      "WARNING:tensorflow:From C:\\Users\\Wen\\miniconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow_core\\python\\training\\input.py:373: range_input_producer (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.range(limit).shuffle(limit).repeat(num_epochs)`. If `shuffle=False`, omit the `.shuffle(...)`.\n",
      "WARNING:tensorflow:From C:\\Users\\Wen\\miniconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow_core\\python\\training\\input.py:319: input_producer (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.from_tensor_slices(input_tensor).shuffle(tf.shape(input_tensor, out_type=tf.int64)[0]).repeat(num_epochs)`. If `shuffle=False`, omit the `.shuffle(...)`.\n",
      "WARNING:tensorflow:From C:\\Users\\Wen\\miniconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow_core\\python\\training\\input.py:189: limit_epochs (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.from_tensors(tensor).repeat(num_epochs)`.\n",
      "WARNING:tensorflow:From C:\\Users\\Wen\\miniconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow_core\\python\\training\\input.py:112: RefVariable.count_up_to (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Prefer Dataset.range instead.\n",
      "WARNING:tensorflow:From C:\\Users\\Wen\\miniconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow_core\\python\\ops\\variables.py:2522: count_up_to (from tensorflow.python.ops.state_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Prefer Dataset.range instead.\n",
      "WARNING:tensorflow:From C:\\Users\\Wen\\miniconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow_core\\python\\training\\input.py:198: QueueRunner.__init__ (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "To construct input pipelines, use the `tf.data` module.\n",
      "WARNING:tensorflow:From C:\\Users\\Wen\\miniconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow_core\\python\\training\\input.py:198: add_queue_runner (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "To construct input pipelines, use the `tf.data` module.\n",
      "WARNING:tensorflow:From <ipython-input-3-4f0f08365d52>:98: shuffle_batch (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.shuffle(min_after_dequeue).batch(batch_size)`.\n",
      "data_info.train_shape: (45024, 204)\n",
      "self._train_data: <tf.Variable 'Train_data:0' shape=(45024, 204) dtype=float32_ref>\n",
      "train_epochs: 25\n",
      "train_frames: [<tf.Tensor 'input_producer/GatherV2:0' shape=(204,) dtype=float32>]\n",
      "\n",
      "We apply weight decay\n",
      "network_input: Tensor(\"AE_Variables/add:0\", shape=(128, 204), dtype=float32)\n",
      "FLAGS.frame_size: 204\n",
      "WARNING:tensorflow:From <ipython-input-3-4f0f08365d52>:227: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "network_input: Tensor(\"Valid_batch:0\", shape=(128, 204), dtype=float32)\n",
      "FLAGS.frame_size: 204\n",
      "\n",
      "DAE with the following shape was created :  [204, 180, 204]\n",
      "WARNING:tensorflow:From <ipython-input-2-4465564218fc>:126: start_queue_runners (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "To construct input pipelines, use the `tf.data` module.\n",
      "Initializing variables ...\n",
      "\n",
      "INFO:tensorflow:Restoring parameters from H:\\minjae\\Trying Obama Dataset\\tmp\\MoCap\\chkpts_exp/chkpt-final\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "Restoring from checkpoint failed. This is most likely due to a mismatch between the current graph and the graph from the checkpoint. Please ensure that you have not altered the graph expected based on the checkpoint. Original error:\n\nAssign requires shapes of both tensors to match. lhs shape= [180] rhs shape= [90]\n\t [[node Train/save/Assign_1 (defined at C:\\Users\\Wen\\miniconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow_core\\python\\framework\\ops.py:1748) ]]\n\nOriginal stack trace for 'Train/save/Assign_1':\n  File \"C:\\Users\\Wen\\miniconda3\\envs\\tensorflow\\lib\\runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"C:\\Users\\Wen\\miniconda3\\envs\\tensorflow\\lib\\runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"C:\\Users\\Wen\\miniconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"C:\\Users\\Wen\\miniconda3\\envs\\tensorflow\\lib\\site-packages\\traitlets\\config\\application.py\", line 845, in launch_instance\n    app.start()\n  File \"C:\\Users\\Wen\\miniconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 612, in start\n    self.io_loop.start()\n  File \"C:\\Users\\Wen\\miniconda3\\envs\\tensorflow\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 199, in start\n    self.asyncio_loop.run_forever()\n  File \"C:\\Users\\Wen\\miniconda3\\envs\\tensorflow\\lib\\asyncio\\base_events.py\", line 541, in run_forever\n    self._run_once()\n  File \"C:\\Users\\Wen\\miniconda3\\envs\\tensorflow\\lib\\asyncio\\base_events.py\", line 1786, in _run_once\n    handle._run()\n  File \"C:\\Users\\Wen\\miniconda3\\envs\\tensorflow\\lib\\asyncio\\events.py\", line 88, in _run\n    self._context.run(self._callback, *self._args)\n  File \"C:\\Users\\Wen\\miniconda3\\envs\\tensorflow\\lib\\site-packages\\tornado\\ioloop.py\", line 688, in <lambda>\n    lambda f: self._run_callback(functools.partial(callback, future))\n  File \"C:\\Users\\Wen\\miniconda3\\envs\\tensorflow\\lib\\site-packages\\tornado\\ioloop.py\", line 741, in _run_callback\n    ret = callback()\n  File \"C:\\Users\\Wen\\miniconda3\\envs\\tensorflow\\lib\\site-packages\\tornado\\gen.py\", line 814, in inner\n    self.ctx_run(self.run)\n  File \"C:\\Users\\Wen\\miniconda3\\envs\\tensorflow\\lib\\site-packages\\tornado\\gen.py\", line 775, in run\n    yielded = self.gen.send(value)\n  File \"C:\\Users\\Wen\\miniconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 381, in dispatch_queue\n    yield self.process_one()\n  File \"C:\\Users\\Wen\\miniconda3\\envs\\tensorflow\\lib\\site-packages\\tornado\\gen.py\", line 250, in wrapper\n    runner = Runner(ctx_run, result, future, yielded)\n  File \"C:\\Users\\Wen\\miniconda3\\envs\\tensorflow\\lib\\site-packages\\tornado\\gen.py\", line 741, in __init__\n    self.ctx_run(self.run)\n  File \"C:\\Users\\Wen\\miniconda3\\envs\\tensorflow\\lib\\site-packages\\tornado\\gen.py\", line 775, in run\n    yielded = self.gen.send(value)\n  File \"C:\\Users\\Wen\\miniconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 365, in process_one\n    yield gen.maybe_future(dispatch(*args))\n  File \"C:\\Users\\Wen\\miniconda3\\envs\\tensorflow\\lib\\site-packages\\tornado\\gen.py\", line 234, in wrapper\n    yielded = ctx_run(next, result)\n  File \"C:\\Users\\Wen\\miniconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 268, in dispatch_shell\n    yield gen.maybe_future(handler(stream, idents, msg))\n  File \"C:\\Users\\Wen\\miniconda3\\envs\\tensorflow\\lib\\site-packages\\tornado\\gen.py\", line 234, in wrapper\n    yielded = ctx_run(next, result)\n  File \"C:\\Users\\Wen\\miniconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 545, in execute_request\n    user_expressions, allow_stdin,\n  File \"C:\\Users\\Wen\\miniconda3\\envs\\tensorflow\\lib\\site-packages\\tornado\\gen.py\", line 234, in wrapper\n    yielded = ctx_run(next, result)\n  File \"C:\\Users\\Wen\\miniconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 306, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"C:\\Users\\Wen\\miniconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 536, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"C:\\Users\\Wen\\miniconda3\\envs\\tensorflow\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2895, in run_cell\n    raw_cell, store_history, silent, shell_futures)\n  File \"C:\\Users\\Wen\\miniconda3\\envs\\tensorflow\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2940, in _run_cell\n    return runner(coro)\n  File \"C:\\Users\\Wen\\miniconda3\\envs\\tensorflow\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 68, in _pseudo_sync_runner\n    coro.send(None)\n  File \"C:\\Users\\Wen\\miniconda3\\envs\\tensorflow\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3166, in run_cell_async\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"C:\\Users\\Wen\\miniconda3\\envs\\tensorflow\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3357, in run_ast_nodes\n    if (await self.run_code(code, result,  async_=asy)):\n  File \"C:\\Users\\Wen\\miniconda3\\envs\\tensorflow\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3437, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-14-ec6f8b4d025b>\", line 10, in <module>\n    nn = create_nn(Y_train_normalized, Y_dev_normalized, max_val, mean_pose, restoring=True)\n  File \"<ipython-input-6-68de15b2c983>\", line 45, in create_nn\n    nn = learning(data, data_info, just_restore=restoring)\n  File \"<ipython-input-2-4465564218fc>\", line 151, in learning\n    saver = tf.train.Saver(write_version=tf.train.SaverDef.V2)\n  File \"C:\\Users\\Wen\\miniconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow_core\\python\\training\\saver.py\", line 828, in __init__\n    self.build()\n  File \"C:\\Users\\Wen\\miniconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow_core\\python\\training\\saver.py\", line 840, in build\n    self._build(self._filename, build_save=True, build_restore=True)\n  File \"C:\\Users\\Wen\\miniconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow_core\\python\\training\\saver.py\", line 878, in _build\n    build_restore=build_restore)\n  File \"C:\\Users\\Wen\\miniconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow_core\\python\\training\\saver.py\", line 508, in _build_internal\n    restore_sequentially, reshape)\n  File \"C:\\Users\\Wen\\miniconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow_core\\python\\training\\saver.py\", line 350, in _AddRestoreOps\n    assign_ops.append(saveable.restore(saveable_tensors, shapes))\n  File \"C:\\Users\\Wen\\miniconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow_core\\python\\training\\saving\\saveable_object_util.py\", line 73, in restore\n    self.op.get_shape().is_fully_defined())\n  File \"C:\\Users\\Wen\\miniconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow_core\\python\\ops\\state_ops.py\", line 227, in assign\n    validate_shape=validate_shape)\n  File \"C:\\Users\\Wen\\miniconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow_core\\python\\ops\\gen_state_ops.py\", line 66, in assign\n    use_locking=use_locking, name=name)\n  File \"C:\\Users\\Wen\\miniconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow_core\\python\\framework\\op_def_library.py\", line 794, in _apply_op_helper\n    op_def=op_def)\n  File \"C:\\Users\\Wen\\miniconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow_core\\python\\util\\deprecation.py\", line 507, in new_func\n    return func(*args, **kwargs)\n  File \"C:\\Users\\Wen\\miniconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow_core\\python\\framework\\ops.py\", line 3357, in create_op\n    attrs, op_def, compute_device)\n  File \"C:\\Users\\Wen\\miniconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow_core\\python\\framework\\ops.py\", line 3426, in _create_op_internal\n    op_def=op_def)\n  File \"C:\\Users\\Wen\\miniconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow_core\\python\\framework\\ops.py\", line 1748, in __init__\n    self._traceback = tf_stack.extract_stack()\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\Wen\\miniconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow_core\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1364\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1365\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1366\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Wen\\miniconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow_core\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1349\u001b[0m       return self._call_tf_sessionrun(options, feed_dict, fetch_list,\n\u001b[1;32m-> 1350\u001b[1;33m                                       target_list, run_metadata)\n\u001b[0m\u001b[0;32m   1351\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Wen\\miniconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow_core\\python\\client\\session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[1;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[0;32m   1442\u001b[0m                                             \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1443\u001b[1;33m                                             run_metadata)\n\u001b[0m\u001b[0;32m   1444\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m: Assign requires shapes of both tensors to match. lhs shape= [180] rhs shape= [90]\n\t [[{{node Train/save/Assign_1}}]]",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\Wen\\miniconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow_core\\python\\training\\saver.py\u001b[0m in \u001b[0;36mrestore\u001b[1;34m(self, sess, save_path)\u001b[0m\n\u001b[0;32m   1289\u001b[0m         sess.run(self.saver_def.restore_op_name,\n\u001b[1;32m-> 1290\u001b[1;33m                  {self.saver_def.filename_tensor_name: save_path})\n\u001b[0m\u001b[0;32m   1291\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mNotFoundError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Wen\\miniconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow_core\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    955\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 956\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    957\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Wen\\miniconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow_core\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1179\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1180\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1181\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Wen\\miniconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow_core\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1358\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[1;32m-> 1359\u001b[1;33m                            run_metadata)\n\u001b[0m\u001b[0;32m   1360\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Wen\\miniconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow_core\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1383\u001b[0m                     'disable_meta_optimizer = True')\n\u001b[1;32m-> 1384\u001b[1;33m       \u001b[1;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1385\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m: Assign requires shapes of both tensors to match. lhs shape= [180] rhs shape= [90]\n\t [[node Train/save/Assign_1 (defined at C:\\Users\\Wen\\miniconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow_core\\python\\framework\\ops.py:1748) ]]\n\nOriginal stack trace for 'Train/save/Assign_1':\n  File \"C:\\Users\\Wen\\miniconda3\\envs\\tensorflow\\lib\\runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"C:\\Users\\Wen\\miniconda3\\envs\\tensorflow\\lib\\runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"C:\\Users\\Wen\\miniconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"C:\\Users\\Wen\\miniconda3\\envs\\tensorflow\\lib\\site-packages\\traitlets\\config\\application.py\", line 845, in launch_instance\n    app.start()\n  File \"C:\\Users\\Wen\\miniconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 612, in start\n    self.io_loop.start()\n  File \"C:\\Users\\Wen\\miniconda3\\envs\\tensorflow\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 199, in start\n    self.asyncio_loop.run_forever()\n  File \"C:\\Users\\Wen\\miniconda3\\envs\\tensorflow\\lib\\asyncio\\base_events.py\", line 541, in run_forever\n    self._run_once()\n  File \"C:\\Users\\Wen\\miniconda3\\envs\\tensorflow\\lib\\asyncio\\base_events.py\", line 1786, in _run_once\n    handle._run()\n  File \"C:\\Users\\Wen\\miniconda3\\envs\\tensorflow\\lib\\asyncio\\events.py\", line 88, in _run\n    self._context.run(self._callback, *self._args)\n  File \"C:\\Users\\Wen\\miniconda3\\envs\\tensorflow\\lib\\site-packages\\tornado\\ioloop.py\", line 688, in <lambda>\n    lambda f: self._run_callback(functools.partial(callback, future))\n  File \"C:\\Users\\Wen\\miniconda3\\envs\\tensorflow\\lib\\site-packages\\tornado\\ioloop.py\", line 741, in _run_callback\n    ret = callback()\n  File \"C:\\Users\\Wen\\miniconda3\\envs\\tensorflow\\lib\\site-packages\\tornado\\gen.py\", line 814, in inner\n    self.ctx_run(self.run)\n  File \"C:\\Users\\Wen\\miniconda3\\envs\\tensorflow\\lib\\site-packages\\tornado\\gen.py\", line 775, in run\n    yielded = self.gen.send(value)\n  File \"C:\\Users\\Wen\\miniconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 381, in dispatch_queue\n    yield self.process_one()\n  File \"C:\\Users\\Wen\\miniconda3\\envs\\tensorflow\\lib\\site-packages\\tornado\\gen.py\", line 250, in wrapper\n    runner = Runner(ctx_run, result, future, yielded)\n  File \"C:\\Users\\Wen\\miniconda3\\envs\\tensorflow\\lib\\site-packages\\tornado\\gen.py\", line 741, in __init__\n    self.ctx_run(self.run)\n  File \"C:\\Users\\Wen\\miniconda3\\envs\\tensorflow\\lib\\site-packages\\tornado\\gen.py\", line 775, in run\n    yielded = self.gen.send(value)\n  File \"C:\\Users\\Wen\\miniconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 365, in process_one\n    yield gen.maybe_future(dispatch(*args))\n  File \"C:\\Users\\Wen\\miniconda3\\envs\\tensorflow\\lib\\site-packages\\tornado\\gen.py\", line 234, in wrapper\n    yielded = ctx_run(next, result)\n  File \"C:\\Users\\Wen\\miniconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 268, in dispatch_shell\n    yield gen.maybe_future(handler(stream, idents, msg))\n  File \"C:\\Users\\Wen\\miniconda3\\envs\\tensorflow\\lib\\site-packages\\tornado\\gen.py\", line 234, in wrapper\n    yielded = ctx_run(next, result)\n  File \"C:\\Users\\Wen\\miniconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 545, in execute_request\n    user_expressions, allow_stdin,\n  File \"C:\\Users\\Wen\\miniconda3\\envs\\tensorflow\\lib\\site-packages\\tornado\\gen.py\", line 234, in wrapper\n    yielded = ctx_run(next, result)\n  File \"C:\\Users\\Wen\\miniconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 306, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"C:\\Users\\Wen\\miniconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 536, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"C:\\Users\\Wen\\miniconda3\\envs\\tensorflow\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2895, in run_cell\n    raw_cell, store_history, silent, shell_futures)\n  File \"C:\\Users\\Wen\\miniconda3\\envs\\tensorflow\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2940, in _run_cell\n    return runner(coro)\n  File \"C:\\Users\\Wen\\miniconda3\\envs\\tensorflow\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 68, in _pseudo_sync_runner\n    coro.send(None)\n  File \"C:\\Users\\Wen\\miniconda3\\envs\\tensorflow\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3166, in run_cell_async\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"C:\\Users\\Wen\\miniconda3\\envs\\tensorflow\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3357, in run_ast_nodes\n    if (await self.run_code(code, result,  async_=asy)):\n  File \"C:\\Users\\Wen\\miniconda3\\envs\\tensorflow\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3437, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-14-ec6f8b4d025b>\", line 10, in <module>\n    nn = create_nn(Y_train_normalized, Y_dev_normalized, max_val, mean_pose, restoring=True)\n  File \"<ipython-input-6-68de15b2c983>\", line 45, in create_nn\n    nn = learning(data, data_info, just_restore=restoring)\n  File \"<ipython-input-2-4465564218fc>\", line 151, in learning\n    saver = tf.train.Saver(write_version=tf.train.SaverDef.V2)\n  File \"C:\\Users\\Wen\\miniconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow_core\\python\\training\\saver.py\", line 828, in __init__\n    self.build()\n  File \"C:\\Users\\Wen\\miniconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow_core\\python\\training\\saver.py\", line 840, in build\n    self._build(self._filename, build_save=True, build_restore=True)\n  File \"C:\\Users\\Wen\\miniconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow_core\\python\\training\\saver.py\", line 878, in _build\n    build_restore=build_restore)\n  File \"C:\\Users\\Wen\\miniconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow_core\\python\\training\\saver.py\", line 508, in _build_internal\n    restore_sequentially, reshape)\n  File \"C:\\Users\\Wen\\miniconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow_core\\python\\training\\saver.py\", line 350, in _AddRestoreOps\n    assign_ops.append(saveable.restore(saveable_tensors, shapes))\n  File \"C:\\Users\\Wen\\miniconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow_core\\python\\training\\saving\\saveable_object_util.py\", line 73, in restore\n    self.op.get_shape().is_fully_defined())\n  File \"C:\\Users\\Wen\\miniconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow_core\\python\\ops\\state_ops.py\", line 227, in assign\n    validate_shape=validate_shape)\n  File \"C:\\Users\\Wen\\miniconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow_core\\python\\ops\\gen_state_ops.py\", line 66, in assign\n    use_locking=use_locking, name=name)\n  File \"C:\\Users\\Wen\\miniconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow_core\\python\\framework\\op_def_library.py\", line 794, in _apply_op_helper\n    op_def=op_def)\n  File \"C:\\Users\\Wen\\miniconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow_core\\python\\util\\deprecation.py\", line 507, in new_func\n    return func(*args, **kwargs)\n  File \"C:\\Users\\Wen\\miniconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow_core\\python\\framework\\ops.py\", line 3357, in create_op\n    attrs, op_def, compute_device)\n  File \"C:\\Users\\Wen\\miniconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow_core\\python\\framework\\ops.py\", line 3426, in _create_op_internal\n    op_def=op_def)\n  File \"C:\\Users\\Wen\\miniconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow_core\\python\\framework\\ops.py\", line 1748, in __init__\n    self._traceback = tf_stack.extract_stack()\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-ec6f8b4d025b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;31m# Train the network\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m \u001b[0mnn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcreate_nn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mY_train_normalized\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_dev_normalized\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmean_pose\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrestoring\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;31m# # Read the encoding\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-6-68de15b2c983>\u001b[0m in \u001b[0;36mcreate_nn\u001b[1;34m(train_data, dev_data, max_val, mean_pose, restoring)\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     44\u001b[0m     \u001b[1;31m# Train the network\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 45\u001b[1;33m     \u001b[0mnn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlearning\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata_info\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mjust_restore\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mrestoring\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     46\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     47\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mnn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-2-4465564218fc>\u001b[0m in \u001b[0;36mlearning\u001b[1;34m(data, data_info, just_restore)\u001b[0m\n\u001b[0;32m    154\u001b[0m             \u001b[1;31m# restore model, if needed\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    155\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mFLAGS\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrestore\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 156\u001b[1;33m                 \u001b[0msaver\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrestore\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msess\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mchkpt_file\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    157\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Model restored from the file \"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mchkpt_file\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'.'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    158\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Wen\\miniconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow_core\\python\\training\\saver.py\u001b[0m in \u001b[0;36mrestore\u001b[1;34m(self, sess, save_path)\u001b[0m\n\u001b[0;32m   1324\u001b[0m       \u001b[1;31m# We add a more reasonable error message here to help users (b/110263146)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1325\u001b[0m       raise _wrap_restore_error_with_msg(\n\u001b[1;32m-> 1326\u001b[1;33m           err, \"a mismatch between the current graph and the graph\")\n\u001b[0m\u001b[0;32m   1327\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1328\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m: Restoring from checkpoint failed. This is most likely due to a mismatch between the current graph and the graph from the checkpoint. Please ensure that you have not altered the graph expected based on the checkpoint. Original error:\n\nAssign requires shapes of both tensors to match. lhs shape= [180] rhs shape= [90]\n\t [[node Train/save/Assign_1 (defined at C:\\Users\\Wen\\miniconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow_core\\python\\framework\\ops.py:1748) ]]\n\nOriginal stack trace for 'Train/save/Assign_1':\n  File \"C:\\Users\\Wen\\miniconda3\\envs\\tensorflow\\lib\\runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"C:\\Users\\Wen\\miniconda3\\envs\\tensorflow\\lib\\runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"C:\\Users\\Wen\\miniconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"C:\\Users\\Wen\\miniconda3\\envs\\tensorflow\\lib\\site-packages\\traitlets\\config\\application.py\", line 845, in launch_instance\n    app.start()\n  File \"C:\\Users\\Wen\\miniconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 612, in start\n    self.io_loop.start()\n  File \"C:\\Users\\Wen\\miniconda3\\envs\\tensorflow\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 199, in start\n    self.asyncio_loop.run_forever()\n  File \"C:\\Users\\Wen\\miniconda3\\envs\\tensorflow\\lib\\asyncio\\base_events.py\", line 541, in run_forever\n    self._run_once()\n  File \"C:\\Users\\Wen\\miniconda3\\envs\\tensorflow\\lib\\asyncio\\base_events.py\", line 1786, in _run_once\n    handle._run()\n  File \"C:\\Users\\Wen\\miniconda3\\envs\\tensorflow\\lib\\asyncio\\events.py\", line 88, in _run\n    self._context.run(self._callback, *self._args)\n  File \"C:\\Users\\Wen\\miniconda3\\envs\\tensorflow\\lib\\site-packages\\tornado\\ioloop.py\", line 688, in <lambda>\n    lambda f: self._run_callback(functools.partial(callback, future))\n  File \"C:\\Users\\Wen\\miniconda3\\envs\\tensorflow\\lib\\site-packages\\tornado\\ioloop.py\", line 741, in _run_callback\n    ret = callback()\n  File \"C:\\Users\\Wen\\miniconda3\\envs\\tensorflow\\lib\\site-packages\\tornado\\gen.py\", line 814, in inner\n    self.ctx_run(self.run)\n  File \"C:\\Users\\Wen\\miniconda3\\envs\\tensorflow\\lib\\site-packages\\tornado\\gen.py\", line 775, in run\n    yielded = self.gen.send(value)\n  File \"C:\\Users\\Wen\\miniconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 381, in dispatch_queue\n    yield self.process_one()\n  File \"C:\\Users\\Wen\\miniconda3\\envs\\tensorflow\\lib\\site-packages\\tornado\\gen.py\", line 250, in wrapper\n    runner = Runner(ctx_run, result, future, yielded)\n  File \"C:\\Users\\Wen\\miniconda3\\envs\\tensorflow\\lib\\site-packages\\tornado\\gen.py\", line 741, in __init__\n    self.ctx_run(self.run)\n  File \"C:\\Users\\Wen\\miniconda3\\envs\\tensorflow\\lib\\site-packages\\tornado\\gen.py\", line 775, in run\n    yielded = self.gen.send(value)\n  File \"C:\\Users\\Wen\\miniconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 365, in process_one\n    yield gen.maybe_future(dispatch(*args))\n  File \"C:\\Users\\Wen\\miniconda3\\envs\\tensorflow\\lib\\site-packages\\tornado\\gen.py\", line 234, in wrapper\n    yielded = ctx_run(next, result)\n  File \"C:\\Users\\Wen\\miniconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 268, in dispatch_shell\n    yield gen.maybe_future(handler(stream, idents, msg))\n  File \"C:\\Users\\Wen\\miniconda3\\envs\\tensorflow\\lib\\site-packages\\tornado\\gen.py\", line 234, in wrapper\n    yielded = ctx_run(next, result)\n  File \"C:\\Users\\Wen\\miniconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 545, in execute_request\n    user_expressions, allow_stdin,\n  File \"C:\\Users\\Wen\\miniconda3\\envs\\tensorflow\\lib\\site-packages\\tornado\\gen.py\", line 234, in wrapper\n    yielded = ctx_run(next, result)\n  File \"C:\\Users\\Wen\\miniconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 306, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"C:\\Users\\Wen\\miniconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 536, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"C:\\Users\\Wen\\miniconda3\\envs\\tensorflow\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2895, in run_cell\n    raw_cell, store_history, silent, shell_futures)\n  File \"C:\\Users\\Wen\\miniconda3\\envs\\tensorflow\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2940, in _run_cell\n    return runner(coro)\n  File \"C:\\Users\\Wen\\miniconda3\\envs\\tensorflow\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 68, in _pseudo_sync_runner\n    coro.send(None)\n  File \"C:\\Users\\Wen\\miniconda3\\envs\\tensorflow\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3166, in run_cell_async\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"C:\\Users\\Wen\\miniconda3\\envs\\tensorflow\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3357, in run_ast_nodes\n    if (await self.run_code(code, result,  async_=asy)):\n  File \"C:\\Users\\Wen\\miniconda3\\envs\\tensorflow\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3437, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-14-ec6f8b4d025b>\", line 10, in <module>\n    nn = create_nn(Y_train_normalized, Y_dev_normalized, max_val, mean_pose, restoring=True)\n  File \"<ipython-input-6-68de15b2c983>\", line 45, in create_nn\n    nn = learning(data, data_info, just_restore=restoring)\n  File \"<ipython-input-2-4465564218fc>\", line 151, in learning\n    saver = tf.train.Saver(write_version=tf.train.SaverDef.V2)\n  File \"C:\\Users\\Wen\\miniconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow_core\\python\\training\\saver.py\", line 828, in __init__\n    self.build()\n  File \"C:\\Users\\Wen\\miniconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow_core\\python\\training\\saver.py\", line 840, in build\n    self._build(self._filename, build_save=True, build_restore=True)\n  File \"C:\\Users\\Wen\\miniconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow_core\\python\\training\\saver.py\", line 878, in _build\n    build_restore=build_restore)\n  File \"C:\\Users\\Wen\\miniconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow_core\\python\\training\\saver.py\", line 508, in _build_internal\n    restore_sequentially, reshape)\n  File \"C:\\Users\\Wen\\miniconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow_core\\python\\training\\saver.py\", line 350, in _AddRestoreOps\n    assign_ops.append(saveable.restore(saveable_tensors, shapes))\n  File \"C:\\Users\\Wen\\miniconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow_core\\python\\training\\saving\\saveable_object_util.py\", line 73, in restore\n    self.op.get_shape().is_fully_defined())\n  File \"C:\\Users\\Wen\\miniconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow_core\\python\\ops\\state_ops.py\", line 227, in assign\n    validate_shape=validate_shape)\n  File \"C:\\Users\\Wen\\miniconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow_core\\python\\ops\\gen_state_ops.py\", line 66, in assign\n    use_locking=use_locking, name=name)\n  File \"C:\\Users\\Wen\\miniconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow_core\\python\\framework\\op_def_library.py\", line 794, in _apply_op_helper\n    op_def=op_def)\n  File \"C:\\Users\\Wen\\miniconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow_core\\python\\util\\deprecation.py\", line 507, in new_func\n    return func(*args, **kwargs)\n  File \"C:\\Users\\Wen\\miniconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow_core\\python\\framework\\ops.py\", line 3357, in create_op\n    attrs, op_def, compute_device)\n  File \"C:\\Users\\Wen\\miniconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow_core\\python\\framework\\ops.py\", line 3426, in _create_op_internal\n    op_def=op_def)\n  File \"C:\\Users\\Wen\\miniconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow_core\\python\\framework\\ops.py\", line 1748, in __init__\n    self._traceback = tf_stack.extract_stack()\n"
     ]
    }
   ],
   "source": [
    "DATA_DIR = 'data_3d_cord'\n",
    "TEST_FILE = 'data_3d_cord/test_inputs/predict_audio42_20fps_3d_cord.txt'\n",
    "OUTPUT_FILE = 'data_3d_cord/test_inputs/predict_motion42_20fps_3d_cord.txt'\n",
    "\n",
    "\n",
    "# Get the data\n",
    "Y_train_normalized, Y_train, Y_test_normalized, Y_test, Y_dev_normalized, max_val, mean_pose  = prepare_motion_data(DATA_DIR)\n",
    "\n",
    "# Train the network\n",
    "nn = create_nn(Y_train_normalized, Y_dev_normalized, max_val, mean_pose, restoring=True)\n",
    "\n",
    "# # Read the encoding\n",
    "# encoding = np.loadtxt(TEST_FILE)\n",
    "\n",
    "# print(encoding.shape)\n",
    "\n",
    "# print(\"encoding\", encoding)\n",
    "\n",
    "# # Decode it\n",
    "# decoding = decode(nn, encoding)\n",
    "\n",
    "# print(decoding.shape)\n",
    "\n",
    "# np.savetxt(OUTPUT_FILE, decoding, delimiter = ' ')\n",
    "\n",
    "# # Close Tf session\n",
    "# nn.session.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "blessed-proxy",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "designed-muslim",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1152, 204)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoding.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "naked-model",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7 (tensorflow)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
